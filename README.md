# Olympic Data Analytics - Azure Data Pipeline

This is an end-to-end data analytics project built using Azure services to process and analyze Olympic datasets stored in CSV files. The project follows a full data engineering pipeline, from raw data ingestion to dashboard visualization.

## Key Steps:

Data Ingestion: Azure Data Factory was used to create a pipeline that ingests raw Olympic dataset CSV files and stores them in Azure Data Lake Gen2.

Data Transformation: Azure Databricks was utilized to transform and clean the data, which was then stored back in Azure Data Lake Gen2.

Data Querying: Azure Synapse Analytics was employed to write SQL queries and retrieve insights from the transformed data.

Data Visualization: Power BI was used to build an interactive dashboard, visualizing key insights and trends from the Olympic datasets.

This project was fully self-learned, demonstrating the integration of multiple Azure services in a seamless data pipeline.

## Technologies Used:

Azure Services ( Azure Data Factory, Azure Data Lake Gen2, Databricks, Synapse Analytics )

Power BI


